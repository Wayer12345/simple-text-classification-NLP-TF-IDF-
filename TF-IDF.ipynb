{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импортирование библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T10:52:58.456780Z",
     "start_time": "2020-07-03T10:52:56.311874Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Считывание и предобработка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T10:52:59.034709Z",
     "start_time": "2020-07-03T10:52:58.456780Z"
    }
   },
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset = 'train')\n",
    "test = fetch_20newsgroups(subset = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T10:52:59.046628Z",
     "start_time": "2020-07-03T10:52:59.038633Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучающих текстов:  11314\n",
      "Количество тестовых текстов:  7532\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "Метка:  7\n"
     ]
    }
   ],
   "source": [
    "print('Количество обучающих текстов: ', len(train['data']))\n",
    "print('Количество тестовых текстов: ', len(test['data']))\n",
    "print()\n",
    "print(train['data'][0].strip())\n",
    "\n",
    "print()\n",
    "print('Метка: ',train['target'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация, удаление слишком коротких слов, удаление стоп-слов, лемматизация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T10:52:59.516918Z",
     "start_time": "2020-07-03T10:52:59.048629Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizer(text):\n",
    "    txt = text.lower()\n",
    "    token_re = re.compile(r\"\\d+[s]+|[-]?\\d+[.,]?\\d+|\\w+[-]+\\w+[-]?\\w+|\\w+[']?\\w+|\\w+\")\n",
    "    txt_tokens = token_re.findall(txt)\n",
    "    tokens = [token for token in txt_tokens if len(token) >= 4]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    token_num = 0\n",
    "    for token in tokens:\n",
    "        lemmatize_token = lemmatizer.lemmatize(token)\n",
    "        tokens[token_num] = lemmatize_token\n",
    "        token_num += 1\n",
    "    \n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if not token in english_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторизация модели при помощи TF-IDF, классификация логистической регрессией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T10:53:53.613631Z",
     "start_time": "2020-07-03T10:52:59.517285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorize',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.7, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_patte...\n",
       "                                 tokenizer=<function tokenizer at 0x000001D0EBC6E9D8>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('classification',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = Pipeline((('vectorize', TfidfVectorizer(tokenizer = tokenizer, max_df = 0.7)),\n",
    "                 ('classification', LogisticRegression())))\n",
    "\n",
    "model.fit(train['data'], train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T10:54:18.579281Z",
     "start_time": "2020-07-03T10:53:53.615632Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность предсказаний на тренировочной выборке:  0.9776383242001061\n",
      "\n",
      "Точность предсказаний на тренировочной выборке:  0.8224907063197026\n"
     ]
    }
   ],
   "source": [
    "train_predictions = model.predict(train['data'])\n",
    "print('Точность предсказаний на тренировочной выборке: ', accuracy_score(train['target'], train_predictions))\n",
    "\n",
    "print()\n",
    "\n",
    "test_predictions = model.predict(test['data'])\n",
    "print('Точность предсказаний на тренировочной выборке: ', accuracy_score(test['target'], test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Модель переобучилась, но все равно выдает достаточно неплохой результат на тестовой выборке. Где-то 4 из 5 текстов классификатор правильно классифицирует.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
